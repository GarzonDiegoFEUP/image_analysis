{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob as gl\n",
    "import cv2\n",
    "import numpy as np\n",
    "def dummy_npwarn_decorator_factory():\n",
    "  def npwarn_decorator(x):\n",
    "    return x\n",
    "  return npwarn_decorator\n",
    "np._no_nep50_warning = getattr(np, '_no_nep50_warning', dummy_npwarn_decorator_factory)\n",
    "\n",
    "import pylab\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = gl.glob('data/raw data/*.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ResizeWithAspectRatio(image, width=None, height=None, inter=cv2.INTER_AREA):\n",
    "    dim = None\n",
    "    (h, w) = image.shape[:2]\n",
    "\n",
    "    if width is None and height is None:\n",
    "        return image\n",
    "    if width is None:\n",
    "        r = height / float(h)\n",
    "        dim = (int(w * r), height)\n",
    "    else:\n",
    "        r = width / float(w)\n",
    "        dim = (width, int(h * r))\n",
    "\n",
    "    return cv2.resize(image, dim, interpolation=inter)\n",
    "\n",
    "\n",
    "# Kmeans \n",
    "def kmeans_color_quantization(image, clusters=8, rounds=1):\n",
    "    h, w = image.shape[:2]\n",
    "    samples = np.zeros([h*w,3], dtype=np.float32)\n",
    "    count = 0\n",
    "\n",
    "    for x in range(h):\n",
    "        for y in range(w):\n",
    "            samples[count] = image[x][y]\n",
    "            count += 1\n",
    "\n",
    "    compactness, labels, centers = cv2.kmeans(samples,\n",
    "            clusters, \n",
    "            None,\n",
    "            (cv2.TERM_CRITERIA_EPS + cv2.TERM_CRITERIA_MAX_ITER, 10000, 0.0001), \n",
    "            rounds, \n",
    "            cv2.KMEANS_RANDOM_CENTERS)\n",
    "\n",
    "    centers = np.uint8(centers)\n",
    "    res = centers[labels.flatten()]\n",
    "    return res.reshape((image.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(columns=['filename', 'number of particles', 'average particle size', 'image coverage'])\n",
    "\n",
    "particles = []\n",
    "size = []\n",
    "coverage = []\n",
    "\n",
    "for f in files:\n",
    "    # Load image\n",
    "    image = cv2.imread(f)\n",
    "    original = image.copy()\n",
    "\n",
    "    # Perform kmeans color segmentation, grayscale, Otsu's threshold\n",
    "    kmeans = kmeans_color_quantization(image, clusters=2)\n",
    "    gray = cv2.cvtColor(kmeans, cv2.COLOR_BGR2GRAY)\n",
    "    thresh = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)[1]\n",
    "\n",
    "    # Find contours, remove tiny specs using contour area filtering, gather points\n",
    "    points_list = []\n",
    "    size_list = []\n",
    "    cnts, _ = cv2.findContours(thresh, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)[-2:]\n",
    "    AREA_THRESHOLD = 1\n",
    "    image_area = image.shape[0] * image.shape[1]\n",
    "    for c in cnts:\n",
    "        area = cv2.contourArea(c)\n",
    "        if area < AREA_THRESHOLD:\n",
    "            cv2.drawContours(thresh, [c], -1, 0, -1)\n",
    "        else:\n",
    "            (x, y), radius = cv2.minEnclosingCircle(c)\n",
    "            points_list.append((int(x), int(y)))\n",
    "            size_list.append(area)\n",
    "\n",
    "    # Apply mask onto original image\n",
    "    result = cv2.bitwise_and(original, original, mask=thresh)\n",
    "    result[thresh==255] = (36,255,12)\n",
    "\n",
    "    # Overlay on original\n",
    "    original[thresh==255] = (36,255,12)\n",
    "\n",
    "    #print(\"Number of particles: {}\".format(len(points_list)))\n",
    "    #print(\"Average particle size: {:.3f}\".format(sum(size_list)/len(size_list)))\n",
    "    #print(\"Image_coverage: {:.3f}\".format(sum(size_list)/image_area))\n",
    "\n",
    "    particles.append(len(points_list))\n",
    "    size.append(sum(size_list)/len(size_list))\n",
    "    coverage.append(sum(size_list)/image_area)\n",
    "\n",
    "    #save_images\n",
    "    cv2.imwrite('data/processed data/original/{}'.format(f.split('/')[-1]), original)\n",
    "    cv2.imwrite('data/processed data/kmeans/{}'.format(f.split('/')[-1]), kmeans)\n",
    "    cv2.imwrite('data/processed data/thresh/{}'.format(f.split('/')[-1]), thresh)\n",
    "    cv2.imwrite('data/processed data/result/{}'.format(f.split('/')[-1]), result)\n",
    "    cv2.imwrite('data/processed data/gray/{}'.format(f.split('/')[-1]), gray)\n",
    "\n",
    "df['filename'] = [f.split('/')[-1] for f in files]\n",
    "df['number of particles'] = particles\n",
    "df['average particle size'] = size\n",
    "df['image coverage'] = coverage\n",
    "\n",
    "df.to_csv('data/data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "image_analysis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
